{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito Demand Prediction Challenge\n",
    "\n",
    "## Equipe:\n",
    "\n",
    "* Catherine dos Santos Tostes - Aluna de Mestrado PPGI\n",
    "* Jean\n",
    "* Felipe\n",
    "\n",
    "Este relatório tem como objetivo descrever as feautures do arquivo processado, descrever as estratégias no pré-processamento e na classificação dos dados, mostrando as diferenças entre os classificadores utilizados. Este trabalho foi realizado através da participação em uma competição do Kaggle da Avito Demand Prediction Challenge.\n",
    "\n",
    "URL da Competição: https://www.kaggle.com/c/avito-demand-prediction\n",
    "\n",
    "A Avito é um dos maiores sites de anúncios de produtos da Rússia com seções de categorias de diversos segmentos. O desafio da Avito trata de tentar prever se um produto irá ter uma alta demanda ou uma baixa demanda para que os vendedores saibam como otimizar as vendas e indicar aos vendedores o quanto de interesse o produto possa ter, para isso será feito uma análise do conteúdo da descrição dos produtos, aplicação de estratégias e classificadores para mostrar a diferença entre eles.\n",
    "\n",
    "A classificação é uma forma de aplicar a aprendizagem supervisionada (Supervised Learning), ou seja, o algoritmo será treinado através de um conjunto de dados de entrada e irá classificar os dados, através do treinamento o algoritmo irá tentar prever os resultados.\n",
    "\n",
    "## Importando módulos e Bibliotecas\n",
    "\n",
    "Será importado os módulos e bibliotecas que serão necessários para a leitura do arquivo, para o processamento dos dados, a aplicação de métodos e classificadores, cálculos necessários para avaliação e criação de gráficos representativos do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos carregar e ler o arquivo de treinamento e o arquivo de teste que está disponível com o nome de train.csv e test.csv respectivamente, será utilizado o DataFrame do Pandas, que é uma biblioteca para auxiliar na análise de dados tabulares bidimensionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset de treinamento\n",
    "dataset_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o arquivo de treinamento carregado, para se entender os dados será utilizado uma função head() do Pandas para exibir as 3 primeiras linhas do arquivo de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Постельные принадлежности</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кокоби(кокон для сна)</td>\n",
       "      <td>Кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Private</td>\n",
       "      <td>d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...</td>\n",
       "      <td>1008</td>\n",
       "      <td>0.12789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Другое</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Стойка для Одежды</td>\n",
       "      <td>Стойка для одежды, под вешалки. С бутика.</td>\n",
       "      <td>3000</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>Private</td>\n",
       "      <td>79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...</td>\n",
       "      <td>692</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Видео, DVD и Blu-ray плееры</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Philips bluray</td>\n",
       "      <td>В хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>4000</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>Private</td>\n",
       "      <td>b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...</td>\n",
       "      <td>3032</td>\n",
       "      <td>0.43177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id       user_id                region            city  \\\n",
       "0  b912c3c6a6ad  e00f8ff2eaf9  Свердловская область    Екатеринбург   \n",
       "1  2dac0150717d  39aeb48f0017     Самарская область          Самара   \n",
       "2  ba83aefab5dc  91e2f88dd6e3    Ростовская область  Ростов-на-Дону   \n",
       "\n",
       "  parent_category_name               category_name  \\\n",
       "0          Личные вещи  Товары для детей и игрушки   \n",
       "1      Для дома и дачи           Мебель и интерьер   \n",
       "2  Бытовая электроника               Аудио и видео   \n",
       "\n",
       "                       param_1 param_2 param_3                  title  \\\n",
       "0    Постельные принадлежности     NaN     NaN  Кокоби(кокон для сна)   \n",
       "1                       Другое     NaN     NaN      Стойка для Одежды   \n",
       "2  Видео, DVD и Blu-ray плееры     NaN     NaN         Philips bluray   \n",
       "\n",
       "                                         description  price  item_seq_number  \\\n",
       "0  Кокон для сна малыша,пользовались меньше месяц...    400                2   \n",
       "1          Стойка для одежды, под вешалки. С бутика.   3000               19   \n",
       "2  В хорошем состоянии, домашний кинотеатр с blu ...   4000                9   \n",
       "\n",
       "  activation_date user_type  \\\n",
       "0      2017-03-28   Private   \n",
       "1      2017-03-26   Private   \n",
       "2      2017-03-20   Private   \n",
       "\n",
       "                                               image  image_top_1  \\\n",
       "0  d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...         1008   \n",
       "1  79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...          692   \n",
       "2  b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...         3032   \n",
       "\n",
       "   deal_probability  \n",
       "0           0.12789  \n",
       "1           0.00000  \n",
       "2           0.43177  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos saber quais são as colunas do arquivo através da função keys()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'user_id', 'region', 'city', 'parent_category_name',\n",
       "       'category_name', 'param_1', 'param_2', 'param_3', 'title',\n",
       "       'description', 'price', 'item_seq_number', 'activation_date',\n",
       "       'user_type', 'image', 'image_top_1', 'deal_probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos carregar e ler o arquivo de treinamento e o arquivo de teste que está disponível com o nome de train.csv e test.csv respectivamente, será utilizado o DataFrame do Pandas, que é uma biblioteca para auxiliar na análise de dados tabulares bidimensionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset de teste\n",
    "dataset_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se notar que ele está dividido em 18 colunas:\n",
    "\n",
    "* item_id: Contém a String de código de cada item à venda.\n",
    "* user_id: Contém a String de código de cada usuário.\n",
    "* region: De qual região da Rússia o usuário faz parte.\n",
    "* city: De qual cidade da Rússia o usuário faz parte.\n",
    "* parent_category_name: Nome da categoria que o produto faz parte.\n",
    "* category_name: Nome da sub-categoria que o produto faz parte.\n",
    "* param_1: Tipo do produto.\n",
    "* param_2: Tipo do produto.\n",
    "* param_3: Tipo do produto.\n",
    "* title: Título do produto à venda.\n",
    "* description: Descrição do produto à venda.\n",
    "* price: Preço do produto.\n",
    "* item_seq_number: Número de sequência do produto.\n",
    "* activation_date: Data de divulgação do produto.\n",
    "* user_type: Tipo de usuário (se é pessoal ou empresarial).\n",
    "* image: Imagem.\n",
    "* image_top_1: Tamanho da Imagem.\n",
    "* deal_probability: Probabilidade de venda do produto.\n",
    "\n",
    "Serão retirados todos os dados que possam ser considerados irrelevantes para a realização dos processos seguintes. As colunas region, city, param_2, param_3, item_seq_number, user_type, image, image_top_1 serão retiradas para as futuras análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train[['item_id', 'user_id', 'parent_category_name', 'category_name', 'param_1', 'title', 'description', 'price', 'activation_date', 'deal_probability']]\n",
    "dataset_test = dataset_test[['item_id', 'user_id', 'parent_category_name', 'category_name', 'param_1', 'title', 'description', 'price', 'activation_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mais sobre os dados será realizado a contagem das linhas e colunas respectivamente na base de dados de treinamento e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Treinamento:  (1503424, 10)\n",
      "Dataset Teste:  (508438, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Dataset Treinamento: ', dataset_train.shape)\n",
    "print('Dataset Teste: ', dataset_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contagem dos dados não-nulos de cada categoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Treinamento:\n",
      "item_id                 1503424\n",
      "user_id                 1503424\n",
      "parent_category_name    1503424\n",
      "category_name           1503424\n",
      "param_1                 1441848\n",
      "title                   1503424\n",
      "description             1387148\n",
      "price                   1418062\n",
      "activation_date         1503424\n",
      "deal_probability        1503424\n",
      "dtype: int64\n",
      "\n",
      "Dataset Teste:\n",
      "item_id                 508438\n",
      "user_id                 508438\n",
      "parent_category_name    508438\n",
      "category_name           508438\n",
      "param_1                 485528\n",
      "title                   508438\n",
      "description             508438\n",
      "price                   477853\n",
      "activation_date         508438\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Dataset Treinamento:')\n",
    "print(dataset_train.count())\n",
    "print('\\nDataset Teste:')\n",
    "print(dataset_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para apresentação das estatísticas dos dados que ficaram após esse filtro de colunas de interesse para realização do algoritmo de aprendizado, será utilizada a função describe() para se obter a quantidadde total, a média, o sevio padrão, o valor mínimo, os quantis, e o valor máximo da coluna price e deal_probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.418062e+06</td>\n",
       "      <td>1503424.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.167081e+05</td>\n",
       "      <td>0.139131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.689154e+07</td>\n",
       "      <td>0.260079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.300000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000e+03</td>\n",
       "      <td>0.150870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.950101e+10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price  deal_probability\n",
       "count  1.418062e+06    1503424.000000\n",
       "mean   3.167081e+05          0.139131\n",
       "std    6.689154e+07          0.260079\n",
       "min    0.000000e+00          0.000000\n",
       "25%    5.000000e+02          0.000000\n",
       "50%    1.300000e+03          0.000000\n",
       "75%    7.000000e+03          0.150870\n",
       "max    7.950101e+10          1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento\n",
    "\n",
    "Alguns modelos de treinamento precisam ser pré-processados para serem normalizados antes de se realizar algum tipo de análise, para que a aplicação dos métodos de classificação não sejam feitos de forma ineficiente, gerando resultados não satisfatórios.\n",
    "\n",
    "A padronização será realizada por partes.\n",
    "\n",
    "Primeiramente será separado por classes em alta probabilidade, média probabilidade e baixa probabilidade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_probability = []\n",
    "average_probability = []\n",
    "low_probability = []\n",
    "\n",
    "for value in dataset_train['deal_probability']:\n",
    "    if value >= 0.67:\n",
    "        high_probability.append(value)\n",
    "    elif value > 0.33 and value < 0.67:\n",
    "        average_probability.append(value)\n",
    "    else:\n",
    "        low_probability.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o Bag of Words\n",
    "\n",
    "Será retirado do texto todos os sopwords (palavras presentes que não tem relevância para o texto ao qual está inserido) através do módulo nltk será importado para o uso de uma biblioteca em russo com as palavras armazenadas.\n",
    "\n",
    "Agora os dados do título do produto serão tratados através do módulo CountVectorizer do Scikit-Learn.\n",
    "\n",
    "O CountVectorizer transformará as palavras do título para uma forma que os algoritmos de aprendizado de máquina reconhecem, pois eles esperam vetores numéricos, o texto será transformado em uma matriz e será realizado a técnica de bag of words.\n",
    "\n",
    "Este módulo converte uma coleção de documentos de texto em uma matriz de contagens de tokens. Com a frequência de cada String é criado um vetor de frequências para um documento, criando uma amostra multivariada. Transformar uma coleção de documento em vetores numéricos é chamado de vetorização utilizando a estratégia dee bag of words ou bag of n-grams.\n",
    "\n",
    "Cada frequência de ocorrência de token individual (normalizada ou não) é tratada como um recurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1503424, 206935)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [word for word in dataset_train['title'] if word not in stopwords.words('russian')]\n",
    "vector = CountVectorizer(lowercase=True)\n",
    "bag_of_words = vector.fit_transform(filtered_words)\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação dos Classificadores\n",
    "\n",
    "Com o pré-processamento dos dados a serem utilizados, a representação dos documentos agora como uma matriz de frequência de ocorrências, o algoritmo de aprendizado de máquina poderá aplicar classificadores para o treinamento do modelo a ser utilizado.\n",
    "\n",
    "Foram escolhidos para serem utilizados os seguintes classificadores:\n",
    "* Rede Neural\n",
    "* Multinomial Naive Bayes\n",
    "* Support Vector Machine (SVM)\n",
    "\n",
    "Primeiramente será realizado a separação dos dados para treinamento e teste dos arquivos do dataset, através de uma amostra será realizado a aplicação dos classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words\n",
    "y = dataset_train['deal_probability'].values\n",
    "\n",
    "n=100\n",
    "\n",
    "X_train = X[:n]\n",
    "X_test = X[n:]\n",
    "y_train = y[:n]\n",
    "y_test = y[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns modelos de classificadores utilizam apenas valores discretos para suas aplicações, e nossos dados são valores contínuos, então será realizado uma conversão para valor discreto, e quando uma aplicação de modelo necessitá-lo ele será usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_discret = preprocessing.LabelEncoder().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural\n",
    "\n",
    "O algoritmo do Perceptron multi-camadas, aprende um aproximador de função não linear para classificação ou regressão. Através do módulo MLPClassifier que implementa um perceptron multi-camadas através da técnica de Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = None\n",
    "mlp_classifier = MLPClassifier()\n",
    "mlp_classifier.fit(X_train, y_train_discret)\n",
    "mlp_predict = mlp_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o algoritmo da rede neural implementado sobre os dados do dataset, podemos visualizar o resultado do treinamento com o predict e o valor esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor do Predict:  [0 0 0 ..., 0 0 0]\n",
      "Valor Esperado:  [ 0.       0.       0.86521 ...,  0.39569  0.       0.     ]\n"
     ]
    }
   ],
   "source": [
    "print('Valor do Predict: ', mlp_predict)\n",
    "print('Valor Esperado: ', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "O algoritmo é baseado no Teorema de Bayes, com a independência de cada dado, muito utilizada na classificação de texto, com contagens vetoriais de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_naive_bayes = None\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "multinomial_naive_bayes.fit(X_train, y_train_discret)\n",
    "multinomial_predict = multinomial_naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o modelo de classificação Multinomial de Naive Bayes implementado sobre os dados do dataset, podemos visualizar o resultado do treinamento com o predict e o valor esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor do Predict:  [0 0 0 ..., 0 0 0]\n",
      "Valor Esperado:  [ 0.       0.       0.86521 ...,  0.39569  0.       0.     ]\n"
     ]
    }
   ],
   "source": [
    "print('Valor do Predict: ', multinomial_predict)\n",
    "print('Valor Esperado: ', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine = None\n",
    "support_vector_machine = svm.SVC()\n",
    "support_vector_machine.fit(X_train, y_train_discret)\n",
    "svm_predict = support_vector_machine.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o modelo de classificação Support Vector Machine (SVM) implementado sobre os dados do dataset, podemos visualizar o resultado do treinamento com o predict e o valor esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_predict)\n",
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
